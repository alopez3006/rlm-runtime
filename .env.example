# Environment Variables Template

# Copy this file to .env and fill in your values

# ==============================================================================
# Core Configuration
# ==============================================================================

# LLM Backend: litellm, openai, anthropic
# RLM_BACKEND=litellm

# Model identifier (e.g., gpt-4o-mini, claude-3-sonnet-20240229)
# RLM_MODEL=gpt-4o-mini

# Temperature: 0.0-2.0 (lower = more deterministic)
# RLM_TEMPERATURE=0.0

# API Key (or use provider-specific env vars like OPENAI_API_KEY)
# RLM_API_KEY=

# ==============================================================================
# Execution Environment
# ==============================================================================

# Environment: local, docker, wasm
# RLM_ENVIRONMENT=local

# ==============================================================================
# Resource Limits
# ==============================================================================

# Maximum recursion depth
# RLM_MAX_DEPTH=4

# Maximum tool calls per completion
# RLM_MAX_SUBCALLS=12

# Token budget per completion
# RLM_TOKEN_BUDGET=8000

# Maximum tool calls budget
# RLM_TOOL_BUDGET=20

# Timeout in seconds
# RLM_TIMEOUT_SECONDS=120

# ==============================================================================
# Parallel Execution
# ==============================================================================

# Enable parallel tool execution
# RLM_PARALLEL_TOOLS=false

# Maximum concurrent tool executions
# RLM_MAX_PARALLEL=5

# ==============================================================================
# Docker Configuration (when RLM_ENVIRONMENT=docker)
# ==============================================================================

# Docker image to use
# RLM_DOCKER_IMAGE=python:3.11-slim

# CPU limit (e.g., 1.0, 2.0)
# RLM_DOCKER_CPUS=1.0

# Memory limit (e.g., 512m, 1g, 2g)
# RLM_DOCKER_MEMORY=512m

# Disable network access (recommended: true)
# RLM_DOCKER_NETWORK_DISABLED=true

# Docker exec timeout
# RLM_DOCKER_TIMEOUT=30

# ==============================================================================
# Logging Configuration
# ==============================================================================

# Log directory
# RLM_LOG_DIR=./logs

# Verbose output
# RLM_VERBOSE=false

# Log level: DEBUG, INFO, WARNING, ERROR
# RLM_LOG_LEVEL=INFO

# ==============================================================================
# Security Configuration
# ==============================================================================

# Allowed file paths (comma-separated, empty = current directory only)
# RLM_ALLOWED_PATHS=

# ==============================================================================
# Snipara Configuration (Optional but Recommended)
# ==============================================================================

# Snipara API key (get from https://snipara.com/dashboard)
# SNIPARA_API_KEY=

# Snipara project slug
# SNIPARA_PROJECT_SLUG=

# Snipara base URL (default: https://api.snipara.com/mcp)
# SNIPARA_BASE_URL=https://api.snipara.com/mcp

# Enable memory tools (Tier 2: rlm_remember, rlm_recall, etc.)
# RLM_MEMORY_ENABLED=false

# ==============================================================================
# Provider-Specific API Keys (Optional)
# ==============================================================================

# OpenAI
# OPENAI_API_KEY=

# Anthropic
# ANTHROPIC_API_KEY=

# Google Vertex AI
# GOOGLE_API_KEY=
# GOOGLE_APPLICATION_CREDENTIALS=

# AWS Bedrock
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_REGION_NAME=us-east-1

# Azure OpenAI
# AZURE_API_KEY=
# AZURE_API_BASE=
# AZURE_API_VERSION=2023-05-15
