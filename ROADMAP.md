# RLM Runtime Roadmap

This document outlines the development roadmap for RLM Runtime.

## Phase 1: Foundation ‚úÖ

**Status: Complete**

Core functionality for recursive LLM completions with sandboxed execution.

| Feature | Status | Description |
|---------|--------|-------------|
| Orchestrator | ‚úÖ | Recursive completion with depth/token budgets |
| Local REPL | ‚úÖ | RestrictedPython sandboxed execution |
| Docker REPL | ‚úÖ | Isolated container execution |
| LiteLLM Backend | ‚úÖ | Support for 100+ LLM providers |
| Trajectory Logging | ‚úÖ | JSONL execution traces |
| CLI | ‚úÖ | `rlm run`, `rlm init`, `rlm logs`, `rlm doctor` |
| Snipara Integration | ‚úÖ | Context optimization tools |
| MCP Server | ‚úÖ | Claude Desktop/Code integration |
| Multi-Project Support | ‚úÖ | Per-project `rlm.toml` configuration |

---

## Phase 2: Stability & Distribution ‚úÖ

**Status: Complete**

Production-ready release infrastructure.

| Feature | Status | Description |
|---------|--------|-------------|
| CI/CD Pipeline | ‚úÖ | GitHub Actions for tests (Python 3.10-3.12) |
| PyPI Release Workflow | ‚úÖ | Automated publishing via trusted publishing |
| Streaming Support | ‚úÖ | Real-time token streaming via `rlm.stream()` |
| Trajectory Visualizer | ‚úÖ | Streamlit dashboard for debugging |
| Error Handling | ‚úÖ | Custom exception hierarchy |
| Test Coverage 90%+ | üîÑ | Currently at 87% (462 tests) |

---

## Phase 3: Execution Environments

**Status: In Progress**

More isolation and execution options.

| Feature | Status | Description |
|---------|--------|-------------|
| WebAssembly REPL | ‚úÖ | Browser-safe execution via Pyodide |
| Resource Quotas | ‚úÖ | CPU/memory tracking in LocalREPL, limits in DockerREPL |
| Docker Resource Reporting | üîÑ | Report actual usage (not just limits) from containers |
| Remote Execution | ‚è≥ | Execute on RunPod/Modal/Lambda |
| Kubernetes Pods | ‚è≥ | Ephemeral pod execution |

---

## Phase 4: Observability

**Status: In Progress**

Production monitoring and debugging capabilities.

| Feature | Status | Description |
|---------|--------|-------------|
| Cost Tracking | ‚úÖ | Per-model pricing, cost budgets, token breakdown |
| Token Budget Enforcement | ‚úÖ | Now enforced (was configured but not checked) |
| OpenTelemetry | ‚è≥ | Distributed tracing integration |
| Prometheus Metrics | ‚è≥ | Token usage, latency, error rates |
| Alerting | ‚è≥ | Budget exceeded, error rate thresholds |

---

## Phase 5: Tool Ecosystem

**Status: Planned**

Extensible plugin system for community contributions.

| Feature | Status | Description |
|---------|--------|-------------|
| Tool Marketplace | ‚è≥ | Registry of community tools |
| Tool Discovery | ‚è≥ | Auto-detect tools from installed packages |
| Tool Versioning | ‚è≥ | Semantic versioning for tool schemas |
| Tool Testing Framework | ‚è≥ | Framework for testing custom tools |

---

## Phase 6: Enterprise Features

**Status: Planned**

Team and organization support.

| Feature | Status | Description |
|---------|--------|-------------|
| API Server Mode | ‚è≥ | HTTP API for team deployments |
| Authentication | ‚è≥ | API keys, OAuth integration |
| Rate Limiting | ‚è≥ | Per-user/project quotas |
| Audit Logging | ‚è≥ | Compliance-ready execution logs |
| Multi-Tenant | ‚è≥ | Isolated execution per tenant |

---

## Phase 7: Advanced LLM Features

**Status: Planned**

Cutting-edge capabilities.

| Feature | Status | Description |
|---------|--------|-------------|
| Parallel Tool Calls | ‚è≥ | Execute multiple tools concurrently |
| Structured Outputs | ‚è≥ | JSON schema-constrained responses |
| Multi-Modal | ‚è≥ | Image/audio input support |
| Agent Memory | ‚è≥ | Persistent context across sessions |
| Self-Improvement | ‚è≥ | Learn from trajectory feedback |

---

## Phase 8: Sub-LLM Orchestration

**Status: Planned**

Recursive sub-LLM calls within a single completion. The model can delegate focused sub-problems to fresh LLM calls with their own context window and budget. Based on Alex Zhang's RLM paper.

See [docs/sub-llm-orchestration.md](docs/sub-llm-orchestration.md) for full specification.

| Feature | Status | Description |
|---------|--------|-------------|
| `rlm_sub_complete` tool | ‚è≥ | Spawn a sub-LLM call with its own context and budget |
| `rlm_batch_complete` tool | ‚è≥ | Parallel sub-LLM calls with shared budget |
| Auto-context injection | ‚è≥ | Auto-query Snipara for sub-calls with `context_query` parameter |
| Budget inheritance | ‚è≥ | Sub-calls get fraction (50%) of parent's remaining budget |
| Cost guardrails | ‚è≥ | Per-session dollar cap, max sub-calls per turn, depth limits |
| Nested trajectory logging | ‚è≥ | Sub-calls logged as nested entries in JSONL trajectory |

**Prerequisites:** Phase 1 (Orchestrator) ‚úÖ, Phase 4 (Cost Tracking) ‚úÖ, Snipara integration ‚úÖ

---

## Phase 9: Autonomous RLM Agent

**Status: Planned**

Full autonomous agent loop: observe ‚Üí think ‚Üí act ‚Üí terminate. The model explores documentation, writes code, spawns sub-LLM calls, and terminates via FINAL/FINAL_VAR protocol when ready.

See [docs/autonomous-agent.md](docs/autonomous-agent.md) for full specification.

| Feature | Status | Description |
|---------|--------|-------------|
| `AgentRunner` class | ‚è≥ | Main agent loop with configurable limits |
| `FINAL("answer")` protocol | ‚è≥ | Natural language termination signal |
| `FINAL_VAR("var")` protocol | ‚è≥ | Return computed REPL variable as result |
| Iteration budget | ‚è≥ | Max observe-think-act cycles (default: 10) |
| Hard safety limits | ‚è≥ | Absolute caps: 50 iterations, $10, 600s timeout |
| Graceful degradation | ‚è≥ | Force FINAL with best answer when limits hit |
| `rlm agent` CLI command | ‚è≥ | Run agent from command line |
| MCP tools (`agent_run`, `agent_status`, `agent_cancel`) | ‚è≥ | Agent control via MCP |
| Trajectory visualizer extension | ‚è≥ | Agent iteration timeline, cost chart, call tree |
| Deterministic replay tests | ‚è≥ | Record and replay agent trajectories for testing |

**Prerequisites:** Phase 8 (Sub-LLM Orchestration), Snipara REPL Context Bridge ‚úÖ

---

## Legend

| Symbol | Meaning |
|--------|---------|
| ‚úÖ | Complete |
| üîÑ | In Progress |
| ‚è≥ | Planned |

---

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Priority Areas

1. **Sub-LLM Orchestration (Phase 8)** - `rlm_sub_complete` and budget inheritance
2. **Autonomous Agent (Phase 9)** - FINAL/FINAL_VAR protocol and agent loop
3. **Test Coverage** - Push from 87% to 90%+ coverage
4. **Docker Resource Reporting** - Report actual CPU/memory usage from containers
5. **OpenTelemetry Integration** - Distributed tracing for observability
6. **Documentation** - Improve guides and examples

### How to Contribute

1. Fork the repository
2. Create a feature branch
3. Submit a pull request

See [Development](README.md#development) for setup instructions.
